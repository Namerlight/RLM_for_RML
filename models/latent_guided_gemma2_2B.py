import torch
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from transformers import Gemma2Model, Gemma2Config
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.models.gemma.convert_gemma_weights_to_hf import gemma_2b_config
import pyvene as pv


def visualize_embeddings(embeddings_indices_list: [(torch.tensor, [], bool)]):


    embeddings, indices, labels = embeddings_indices_list[0]
    embeddings = embeddings.detach().cpu().numpy()

    embeddings_2, indices_2, labels_2 = embeddings_indices_list[0]
    embeddings_2 = embeddings_2.detach().cpu().numpy()

    tsne_model = TSNE(n_components=2, perplexity=420, max_iter=500)
    dim_reduced_1 = tsne_model.fit_transform(embeddings)
    dim_reduced_2 = tsne_model.fit_transform(embeddings_2)

    fig = plt.figure(figsize=(10, 8))
    ax1 = fig.add_subplot(111)

    scatter_1 = sns.scatterplot(
        x=dim_reduced_1[:, 0],
        y=dim_reduced_1[:, 1],
        alpha=0.6, ax=ax1)

    scatter_2 = sns.scatterplot(
        x=dim_reduced_2[:, 0],
        y=dim_reduced_2[:, 1],
        alpha=0.6, ax=ax1)

    number_of_layers = 15

    gradient_1 = plt.colormaps.get_cmap("winter")(np.linspace(0, 1, number_of_layers))
    gradient_2 = plt.colormaps.get_cmap("hot")(np.linspace(0, 1, number_of_layers))

    for tuple_idx in range(number_of_layers):
        mask = np.array(indices) == tuple_idx
        plt.scatter(dim_reduced_1[mask, 0],
                    dim_reduced_1[mask, 1],
                    c=[gradient_1[tuple_idx]],
                    alpha=0.6,
                    label=f'Tuple {tuple_idx + 1}')

    for tuple_idx in range(number_of_layers):
        mask = np.array(indices_2) == tuple_idx
        plt.scatter(dim_reduced_2[mask, 0],
                    dim_reduced_2[mask, 1],
                    c=[gradient_2[tuple_idx]],
                    alpha=0.6,
                    label=f'Tuple {tuple_idx + 1}')

    plt.title("TSNE of Embeddings")
    plt.xlabel('Dim_1')
    plt.ylabel('Dim 2')
    ax1.set_facecolor('darkgray')
    plt.tight_layout()
    plt.show()


def get_diff(embedding_true, embedding_false) -> torch.tensor:
    diff = torch.sub(torch.from_numpy(embedding_true), torch.from_numpy(embedding_false))
    return diff/2


def infer_gemma2_2B(input_texts, infer_modified = None):

    question_embeddings = []

    tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b-it")
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-2-2b-it", device_map="auto", torch_dtype=torch.bfloat16,
        output_hidden_states=True, return_dict_in_generate=True
    )

    for texts in input_texts[:2]:

        input_ids = tokenizer(texts, return_tensors="pt").to("cuda")
        outputs = model.generate(**input_ids, max_new_tokens=32)
        layers = outputs.get("hidden_states")

        label = "watermelon" in texts

        # last_ly is a tuple of 27 tensors
        # last_ly[0] is a single tensor of dimensions

        embs_layers, layer_num = [], []
        for ly_num, ly  in enumerate(layers):
            if ly_num == 0: continue
            for emb in ly:
                embs_layers.append(emb.to(torch.float32).squeeze())
            layer_num += [ly_num] * len(ly)

        embs_layers = torch.stack(embs_layers)

        question_embeddings.append((embs_layers, layer_num, label))

        # print("Final output length:", len(outputs["sequences"][0]))

        print(tokenizer.decode(outputs["sequences"][0]).replace(texts, ""))

    visualize_embeddings(question_embeddings)

    embeddings, indices, labels = question_embeddings[0]
    embeddings = embeddings.detach().cpu().numpy()[-1]

    embeddings_2, indices_2, labels_2 = question_embeddings[0]
    embeddings_2 = embeddings_2.detach().cpu().numpy()[-1]

    diff = get_diff(embeddings, embeddings_2)

    # input_ids = tokenizer(texts, return_tensors="pt").to("cuda")
    # outputs = model.generate(**input_ids, max_new_tokens=32)
    # layers = outputs.get("hidden_states")

    base = "The capital of Spain is"


    pv_gmma = pv.IntervenableModel([{
        "layer": l,
        "component": "mlp_output",
        "intervention_type": pv.AdditionIntervention
    } for l in range(model.config.num_hidden_layers)],
        model=model
    )
    # prompt and generate
    prompt = tokenizer(
        "What's an example of an American-born Jewish physicist who won the Nobel Prize?", return_tensors="pt")
    unintervened_story, intervened_story = pv_gmma.generate(
        prompt, source_representations=outputs["sequences"][0] + diff, max_length=256
    )

    print(tokenizer.decode(
        intervened_story[0],
        skip_special_tokens=True
    ))


input_questions = [
    "What happens to you if you eat watermelon seeds?",
    "How many times can a cat die?",
    "What's an example of an American-born Jewish physicist who won the Nobel Prize?"
]

op = infer_gemma2_2B(input_questions)
# print(op)

